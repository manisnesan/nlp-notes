{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f1563938438>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f15623b8b88>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f15623b8be8>)]\n"
     ]
    }
   ],
   "source": [
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom components are great for adding custom values to documents, tokens, spans and customizing the `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the custom component\n",
    "def length_component(doc):\n",
    "    #Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    \n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    #Return the doc\n",
    "    return doc\n",
    "\n",
    "#Load the small English mode;\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('This is a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# Write a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "#Load the small English mode;\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "animal_patterns = nlp.pipe(['Golden Retriever', 'cat', 'turtle', 'Rattus norvegicus'])\n",
    "matcher.add('ANIMAL', None, *animal_patterns)\n",
    "\n",
    "# Define the custome component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    doc.ents = [Span(doc, start, end, label='ANIMAL')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp('I have a cat and a Golden Retriever')\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add custom metadata to documents, tokens and spans. These are accessible via `._` property. This should be registered on the global `Doc`, `Token` or `Span` using the `set_extension` method.\n",
    "\n",
    "There are 3 types \n",
    "\n",
    "1. Attribute extension - set a default value that can be overwritten\n",
    "2. Property extensions - define a getter & an optional setter. getter only called when you *retrieve* the attrinute value. Span extensions should always use a getter\n",
    "3. Method extensions - assign a **function** that becomes available as an object method. allows you pass **arguments** to the extension function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inaM\n"
     ]
    }
   ],
   "source": [
    "name = 'Mani'\n",
    "print(''.join(reversed(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n",
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    #Alternate version return token.text[::-1]\n",
    "    return ''.join(reversed(token.text))\n",
    "\n",
    "## Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False, force=True)\n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp('I live in Spain.')\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "print([(token.text, token._.is_country) for token in doc])\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token, Doc, Span\n",
    "\n",
    "#Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "        return any(token.like_num for token in doc)\n",
    "    \n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number, force=True)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token, Doc, Span\n",
    "\n",
    "#Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities and extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Define the getter method\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "    \n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components with extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example for adding structured data to the spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = ['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"Côte d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "global COUNTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = {'Åland Islands': 'Mariehamn', 'Lesotho': 'Maseru', 'Trinidad and Tobago': 'Port of Spain', 'Yemen': \"Sana'a\", 'Christmas Island': 'Flying Fish Cove', 'Bouvet Island': '', 'Kyrgyzstan': 'Bishkek', 'New Caledonia': 'Nouméa', 'Hong Kong': 'City of Victoria', 'Uzbekistan': 'Tashkent', 'Cabo Verde': 'Praia', 'Bulgaria': 'Sofia', 'Bahrain': 'Manama', 'Kuwait': 'Kuwait City', 'Bhutan': 'Thimphu', 'Niue': 'Alofi', 'Mauritania': 'Nouakchott', 'French Polynesia': 'Papeetē', 'Sweden': 'Stockholm', 'Latvia': 'Riga', 'Marshall Islands': 'Majuro', 'Sierra Leone': 'Freetown', 'Senegal': 'Dakar', 'Cambodia': 'Phnom Penh', 'Kiribati': 'South Tarawa', 'Kazakhstan': 'Astana', 'San Marino': 'City of San Marino', 'Madagascar': 'Antananarivo', 'Virgin Islands (U.S.)': 'Charlotte Amalie', 'Bermuda': 'Hamilton', 'South Sudan': 'Juba', 'Turkmenistan': 'Ashgabat', \"Côte d'Ivoire\": 'Yamoussoukro', 'Eritrea': 'Asmara', 'Samoa': 'Apia', 'Puerto Rico': 'San Juan', 'Zambia': 'Lusaka', 'Israel': 'Jerusalem', 'Somalia': 'Mogadishu', 'Timor-Leste': 'Dili', 'Central African Republic': 'Bangui', 'Peru': 'Lima', 'Guinea': 'Conakry', 'Palestine, State of': 'Ramallah', 'Mozambique': 'Maputo', 'Guam': 'Hagåtña', 'Isle of Man': 'Douglas', 'Czech Republic': 'Prague', 'Angola': 'Luanda', 'Luxembourg': 'Luxembourg', 'Nicaragua': 'Managua', 'Liberia': 'Monrovia', 'Bangladesh': 'Dhaka', 'Jordan': 'Amman', 'Romania': 'Bucharest', 'Slovakia': 'Bratislava', 'Egypt': 'Cairo', 'French Southern Territories': 'Port-aux-Français', 'Maldives': 'Malé', 'Chile': 'Santiago', 'Uganda': 'Kampala', 'Anguilla': 'The Valley', 'Bonaire, Sint Eustatius and Saba': 'Kralendijk', 'Syrian Arab Republic': 'Damascus', 'Ethiopia': 'Addis Ababa', 'Paraguay': 'Asunción', 'American Samoa': 'Pago Pago', 'Iraq': 'Baghdad', 'Slovenia': 'Ljubljana', 'Malta': 'Valletta', 'Norfolk Island': 'Kingston', 'Finland': 'Helsinki', 'Tajikistan': 'Dushanbe', 'Palau': 'Ngerulmud', 'Nigeria': 'Abuja', 'Fiji': 'Suva', 'Honduras': 'Tegucigalpa', 'Qatar': 'Doha', 'New Zealand': 'Wellington', 'Grenada': \"St. George's\", 'Kenya': 'Nairobi', 'Mauritius': 'Port Louis', 'Cocos (Keeling) Islands': 'West Island', 'Spain': 'Madrid', 'Curaçao': 'Willemstad', 'Guinea-Bissau': 'Bissau', 'Republic of Kosovo': 'Pristina', 'Macedonia (the former Yugoslav Republic of)': 'Skopje', 'Iran (Islamic Republic of)': 'Tehran', 'Poland': 'Warsaw', 'Falkland Islands (Malvinas)': 'Stanley', 'Gibraltar': 'Gibraltar', 'United Kingdom of Great Britain and Northern Ireland': 'London', 'China': 'Beijing', 'Ireland': 'Dublin', 'Sint Maarten (Dutch part)': 'Philipsburg', 'Greenland': 'Nuuk', 'United Arab Emirates': 'Abu Dhabi', 'Nauru': 'Yaren', 'Italy': 'Rome', 'Congo': 'Brazzaville', 'Zimbabwe': 'Harare', 'Réunion': 'Saint-Denis', 'Tanzania, United Republic of': 'Dodoma', \"Lao People's Democratic Republic\": 'Vientiane', 'Svalbard and Jan Mayen': 'Longyearbyen', 'Brazil': 'Brasília', 'Western Sahara': 'El Aaiún', 'Taiwan': 'Taipei', 'Heard Island and McDonald Islands': '', 'Liechtenstein': 'Vaduz', 'Burkina Faso': 'Ouagadougou', 'Philippines': 'Manila', 'Togo': 'Lomé', 'Singapore': 'Singapore', 'Cayman Islands': 'George Town', 'Saint Lucia': 'Castries', 'Saudi Arabia': 'Riyadh', 'Monaco': 'Monaco', 'Cameroon': 'Yaoundé', 'Wallis and Futuna': 'Mata-Utu', 'South Africa': 'Pretoria', 'Costa Rica': 'San José', 'Mexico': 'Mexico City', 'Guadeloupe': 'Basse-Terre', 'Serbia': 'Belgrade', 'Saint Vincent and the Grenadines': 'Kingstown', 'Papua New Guinea': 'Port Moresby', 'United States Minor Outlying Islands': '', 'Rwanda': 'Kigali', 'Suriname': 'Paramaribo', 'Russian Federation': 'Moscow', 'Lebanon': 'Beirut', 'Saint Helena, Ascension and Tristan da Cunha': 'Jamestown', 'Malawi': 'Lilongwe', 'Pakistan': 'Islamabad', 'Namibia': 'Windhoek', 'Niger': 'Niamey', 'France': 'Paris', 'Solomon Islands': 'Honiara', 'Ghana': 'Accra', 'Sudan': 'Khartoum', 'United States of America': 'Washington, D.C.', 'Greece': 'Athens', 'Botswana': 'Gaborone', 'Belgium': 'Brussels', 'Faroe Islands': 'Tórshavn', 'Ukraine': 'Kiev', 'Moldova (Republic of)': 'Chișinău', 'Oman': 'Muscat', \"Korea (Democratic People's Republic of)\": 'Pyongyang', 'Albania': 'Tirana', 'India': 'New Delhi', 'Viet Nam': 'Hanoi', 'Mongolia': 'Ulan Bator', 'Afghanistan': 'Kabul', 'Tokelau': 'Fakaofo', 'Montenegro': 'Podgorica', 'Colombia': 'Bogotá', 'Equatorial Guinea': 'Malabo', 'Croatia': 'Zagreb', 'Cuba': 'Havana', 'Panama': 'Panama City', 'Cyprus': 'Nicosia', 'Burundi': 'Bujumbura', 'Canada': 'Ottawa', 'Morocco': 'Rabat', 'Virgin Islands (British)': 'Road Town', 'Indonesia': 'Jakarta', 'Tunisia': 'Tunis', 'Ecuador': 'Quito', 'Libya': 'Tripoli', 'Barbados': 'Bridgetown', 'Seychelles': 'Victoria', 'Brunei Darussalam': 'Bandar Seri Begawan', 'Lithuania': 'Vilnius', 'Congo (Democratic Republic of the)': 'Kinshasa', 'Bolivia (Plurinational State of)': 'Sucre', 'Norway': 'Oslo', 'Swaziland': 'Lobamba', 'Australia': 'Canberra', 'Benin': 'Porto-Novo', 'Mayotte': 'Mamoudzou', 'Turkey': 'Ankara', 'Holy See': 'Rome', 'Dominican Republic': 'Santo Domingo', 'Andorra': 'Andorra la Vella', 'Dominica': 'Roseau', 'Montserrat': 'Plymouth', 'Vanuatu': 'Port Vila', 'Jersey': 'Saint Helier', 'Gabon': 'Libreville', 'Bosnia and Herzegovina': 'Sarajevo', 'Antarctica': '', 'Japan': 'Tokyo', 'Turks and Caicos Islands': 'Cockburn Town', 'Saint Pierre and Miquelon': 'Saint-Pierre', 'Thailand': 'Bangkok', 'Bahamas': 'Nassau', 'Sri Lanka': 'Colombo', 'Tonga': \"Nuku'alofa\", 'Korea (Republic of)': 'Seoul', 'Argentina': 'Buenos Aires', 'British Indian Ocean Territory': 'Diego Garcia', 'Iceland': 'Reykjavík', 'El Salvador': 'San Salvador', 'Germany': 'Berlin', 'Pitcairn': 'Adamstown', 'Comoros': 'Moroni', 'Azerbaijan': 'Baku', 'Switzerland': 'Bern', 'Georgia': 'Tbilisi', 'Northern Mariana Islands': 'Saipan', 'Malaysia': 'Kuala Lumpur', 'Aruba': 'Oranjestad', 'Uruguay': 'Montevideo', 'Sao Tome and Principe': 'São Tomé', 'Venezuela (Bolivarian Republic of)': 'Caracas', 'Saint Kitts and Nevis': 'Basseterre', 'South Georgia and the South Sandwich Islands': 'King Edward Point', 'Jamaica': 'Kingston', 'Belarus': 'Minsk', 'Saint Martin (French part)': 'Marigot', 'Portugal': 'Lisbon', 'Guyana': 'Georgetown', 'Martinique': 'Fort-de-France', 'French Guiana': 'Cayenne', 'Cook Islands': 'Avarua', 'Tuvalu': 'Funafuti', 'Estonia': 'Tallinn', 'Antigua and Barbuda': \"Saint John's\", 'Guernsey': 'St. Peter Port', 'Haiti': 'Port-au-Prince', 'Mali': 'Bamako', 'Gambia': 'Banjul', 'Micronesia (Federated States of)': 'Palikir', 'Armenia': 'Yerevan', 'Netherlands': 'Amsterdam', 'Austria': 'Vienna', 'Saint Barthélemy': 'Gustavia', 'Djibouti': 'Djibouti', 'Myanmar': 'Naypyidaw', 'Hungary': 'Budapest', 'Belize': 'Belmopan', 'Denmark': 'Copenhagen', 'Macao': '', 'Nepal': 'Kathmandu', 'Guatemala': 'Guatemala City', 'Chad': \"N'Djamena\", 'Algeria': 'Algiers'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def countries_component(doc):\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    patterns = list(nlp.pipe(COUNTRIES))\n",
    "    matcher.add('COUNTRY', None, *patterns)\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') \n",
    "                for matcher_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "#Add the component to the pipeline\n",
    "#nlp.add_pipe(countries_component)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: capitals.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital \n",
    "Span.set_extension('capital', getter=get_capital, force=True)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing large volumes of text as streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nlp.pipe` method processes texts as stream, yields `Doc` objects\n",
    "- **Bad** : `docs = [nlp(text) for text in LOTS_OF_TEXTS]`\n",
    "- **Good**: `docs = nlp.pipe(LOTS_OF_TEXTS)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['McDonalds is my favorite restaurant.', 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..', 'People really still eat McDonalds :(', 'The McDonalds in Spain has chicken wings. My heart is so happy ', '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P', 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D', 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible', 'gettin', 'payin']\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) (WANT, McRib) (This morning,)\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.ppe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "#Create a list of patterns for the Phrase Matcher\n",
    "patterns = list(nlp.pipe(people))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "- yields `(doc, context)` tuples\n",
    "- useful for associating metadata with `doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.', {'book': 'Metamorphosis', 'author': 'Franz Kafka'}), (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\", {'book': 'Moby-Dick or, The Whale', 'author': 'Herman Melville'}), ('It was the best of times, it was the worst of times.', {'book': 'A Tale of Two Cities', 'author': 'Charles Dickens'}), ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.', {'book': 'On the Road', 'author': 'Jack Kerouac'}), ('It was a bright cold day in April, and the clocks were striking thirteen.', {'book': '1984', 'author': 'George Orwell'}), ('Nowadays people know the price of everything and the value of nothing.', {'book': 'The Picture Of Dorian Gray', 'author': 'Oscar Wilde'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author', default=None, force=True)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book', default=None, force=True)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    #Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same technique is useful for a variety of tasks. For example, you could pass in page or paragraph numbers to relate the processed Doc back to the position in a larger document. Or you could pass in other structured data like IDs referring to a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and updating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try how to update spaCy's statistical models to customize them for our use case – for example, to predict a new entity type in online comments. We will write our own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make our custom NLP projects more successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's components are supervised models for text annotations, meaning they can only learn to reproduce examples, not guess new labels from raw text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy's rule based `Matcher` is a great way to quickly create training data for named entity models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = ['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', 'The iPhone 8 reviews are here', 'Your iPhone goes up to 11 today', 'I need a new phone! Any tips?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#Patterns are the list of dictionaries describing the tokens\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'},\n",
    "            {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'},\n",
    "            {'IS_DIGIT': True, 'OP': '?'}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above patterns to quickly bootstrap some training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Find the matches in the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matches]\n",
    "    \n",
    "    # Get a list of (start, end, label) tuples of matches in the text\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    #print(doc.text, entities)\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created some training examples using the Matcher and with the patterns. Before we train a model with the data, we always want to double-check that the matcher didn't identify any false positives. But that process is still much faster than doing everything manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the spacy pipeline to train the entity recognizer to recognize 'GADGET' entities in a text - for example, \"iPhone X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label('GADGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]}), ('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]}), ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]}), ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]}), ('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}), ('I need a new phone! Any tips?', {'entities': []})]\n"
     ]
    }
   ],
   "source": [
    "print(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 10.400000095367432}\n",
      "{'ner': 21.78433907032013}\n",
      "{'ner': 31.6200110912323}\n",
      "{'ner': 6.9140013456344604}\n",
      "{'ner': 13.714298665523529}\n",
      "{'ner': 18.08437442779541}\n",
      "{'ner': 2.892008237540722}\n",
      "{'ner': 4.254102131351829}\n",
      "{'ner': 7.10178685025312}\n",
      "{'ner': 2.165703824372031}\n",
      "{'ner': 2.9505524254855118}\n",
      "{'ner': 4.399802905296383}\n",
      "{'ner': 0.5348995538088275}\n",
      "{'ner': 1.7707562224250069}\n",
      "{'ner': 8.03996536892464}\n",
      "{'ner': 2.747800108820229}\n",
      "{'ner': 3.0647464738540293}\n",
      "{'ner': 3.388460897228356}\n",
      "{'ner': 0.1266126869886648}\n",
      "{'ner': 1.515483751281863}\n",
      "{'ner': 1.5165760675422888}\n",
      "{'ner': 0.012677417390364099}\n",
      "{'ner': 1.1824903084988136}\n",
      "{'ner': 1.182629649928657}\n",
      "{'ner': 0.7028325000344466}\n",
      "{'ner': 0.7031372110304019}\n",
      "{'ner': 0.7031603734277141}\n",
      "{'ner': 3.566338841665129e-05}\n",
      "{'ner': 1.9963283586681655}\n",
      "{'ner': 1.9963336612291895}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "#Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    #Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    \n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them \n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        #Update the model\n",
    "        nlp.update(texts, annotations, losses = losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the spacy mode. Numbers on the right indicate the loss on each iteration, amount of work left for the optimizer. Lower the number, the better. In real life, we normally want to use a lot more data than this, ideally atleast a few hundred or a few thousand examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it', \"I finally understand what the iPhone X 'notch' is for\", 'Everything you need to know about the Samsung Galaxy S9', 'Looking to compare iPad models? Here’s how the 2018 lineup stacks up', 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple', 'what is the cheapest ipad, especially ipad pro???', 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "(iPhone, iPhone) \n",
      "\n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "(iPhone,) \n",
      "\n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "() \n",
      "\n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "() \n",
      "\n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "(iPhone 8, iPhone 8) \n",
      "\n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "() \n",
      "\n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "() \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each text in TEST_DATA\n",
    "for doc in nlp.pipe(TEST_DATA):\n",
    "    # Print the document text and entitites\n",
    "    print(doc.text)\n",
    "    print(doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"i went to amsterdem last year and the canals were beautiful\", {'entities': [(10, 19, 'GPE')]}),\n",
    "    (\"You should visit Paris once in your life, but the Eiffel Tower is kinda boring\", {'entities': [(17, 22, 'GPE')]}),\n",
    "    (\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24, 32, 'GPE')]}),\n",
    "    (\"Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!\", {'entities': [(0, 6, 'GPE')]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i went to amsterdem last year and the canals were beautiful', {'entities': [(10, 19, 'GPE')]})\n",
      "('You should visit Paris once in your life, but the Eiffel Tower is kinda boring', {'entities': [(17, 22, 'GPE')]})\n",
      "(\"There's also a Paris in Arkansas, lol\", {'entities': [(15, 20, 'GPE'), (24, 32, 'GPE')]})\n",
      "('Berlin is perfect for summer holiday: lots of parks, great nightlife, cheap beer!', {'entities': [(0, 6, 'GPE')]})\n"
     ]
    }
   ],
   "source": [
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\"Reddit partners with Patreon to help creators build communities\", \n",
    "     {'entities': [(0,6, 'WEBSITE'), (21, 28, 'WEBSITE')]}),\n",
    "  \n",
    "    (\"PewDiePie smashes YouTube record\", \n",
    "     {'entities': [(18, 25, 'WEBSITE'), (0, 9,'PERSON')]}),\n",
    "  \n",
    "    (\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\", \n",
    "     {'entities': [(0, 6, 'WEBSITE'), (15, 29,'PERSON')]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PewDiePie smashes YouTube record',\n",
       " {'entities': [(18, 25, 'WEBSITE'), (0, 9, 'PERSON')]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing this labeling by hand. In real life, we probably want to automate this and use an annotation tool - Eg: [Brat](http://brat.nlplab.org/), an open source solution or [Prodigy](https://prodi.gy/), explosion.ai annotation tool that integrates with spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
